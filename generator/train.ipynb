{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('dataset/train.csv')\n",
    "validation_dataset = pd.read_csv('dataset/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private schools generally prefer to be called ...</td>\n",
       "      <td>public</td>\n",
       "      <td>What schools do preparatory schools prepare Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private schools, also known as independent sch...</td>\n",
       "      <td>independent</td>\n",
       "      <td>Along with non-governmental and nonstate schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Pilgrim Street building was refurbished be...</td>\n",
       "      <td>three</td>\n",
       "      <td>How many cinemas are currently housed at one s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chris Keates, the general secretary of Nationa...</td>\n",
       "      <td>child protection and parental rights groups</td>\n",
       "      <td>A statement made by Chris Keates caused issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In Berlin, the Huguenots created two new neigh...</td>\n",
       "      <td>1806-07</td>\n",
       "      <td>What years did this occupation take place?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>Compact trucks were introduced, such as the To...</td>\n",
       "      <td>Dodge D-50</td>\n",
       "      <td>What did Mitsubishi rename its Forte to?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>Luther's rediscovery of \"Christ and His salvat...</td>\n",
       "      <td>Christ and His salvation</td>\n",
       "      <td>What became the foundation of the Reformation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>BSkyB has no veto over the presence of channel...</td>\n",
       "      <td>Ofcom</td>\n",
       "      <td>Who does BSkyB have an operating license from?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9650</th>\n",
       "      <td>Research by Harvard economist Robert Barro, fo...</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>What institution does Robert Barro hail from?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9651</th>\n",
       "      <td>LeGrande writes that \"the formulation of a sin...</td>\n",
       "      <td>specific</td>\n",
       "      <td>Le grand concludes that an author's words offe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9652 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context  \\\n",
       "0     Private schools generally prefer to be called ...   \n",
       "1     Private schools, also known as independent sch...   \n",
       "2     The Pilgrim Street building was refurbished be...   \n",
       "3     Chris Keates, the general secretary of Nationa...   \n",
       "4     In Berlin, the Huguenots created two new neigh...   \n",
       "...                                                 ...   \n",
       "9647  Compact trucks were introduced, such as the To...   \n",
       "9648  Luther's rediscovery of \"Christ and His salvat...   \n",
       "9649  BSkyB has no veto over the presence of channel...   \n",
       "9650  Research by Harvard economist Robert Barro, fo...   \n",
       "9651  LeGrande writes that \"the formulation of a sin...   \n",
       "\n",
       "                                           answer  \\\n",
       "0                                          public   \n",
       "1                                     independent   \n",
       "2                                           three   \n",
       "3     child protection and parental rights groups   \n",
       "4                                         1806-07   \n",
       "...                                           ...   \n",
       "9647                                   Dodge D-50   \n",
       "9648                     Christ and His salvation   \n",
       "9649                                        Ofcom   \n",
       "9650                                      Harvard   \n",
       "9651                                     specific   \n",
       "\n",
       "                                               question  \n",
       "0     What schools do preparatory schools prepare Br...  \n",
       "1     Along with non-governmental and nonstate schoo...  \n",
       "2     How many cinemas are currently housed at one s...  \n",
       "3     A statement made by Chris Keates caused issues...  \n",
       "4            What years did this occupation take place?  \n",
       "...                                                 ...  \n",
       "9647           What did Mitsubishi rename its Forte to?  \n",
       "9648     What became the foundation of the Reformation?  \n",
       "9649     Who does BSkyB have an operating license from?  \n",
       "9650      What institution does Robert Barro hail from?  \n",
       "9651  Le grand concludes that an author's words offe...  \n",
       "\n",
       "[9652 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'context'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "sample_validation_dataset = next(iter(validation_dataset))\n",
    "pprint (sample_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private schools generally prefer to be called ...</td>\n",
       "      <td>public</td>\n",
       "      <td>What schools do preparatory schools prepare Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Private schools, also known as independent sch...</td>\n",
       "      <td>independent</td>\n",
       "      <td>Along with non-governmental and nonstate schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Pilgrim Street building was refurbished be...</td>\n",
       "      <td>three</td>\n",
       "      <td>How many cinemas are currently housed at one s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chris Keates, the general secretary of Nationa...</td>\n",
       "      <td>child protection and parental rights groups</td>\n",
       "      <td>A statement made by Chris Keates caused issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In Berlin, the Huguenots created two new neigh...</td>\n",
       "      <td>1806-07</td>\n",
       "      <td>What years did this occupation take place?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>Compact trucks were introduced, such as the To...</td>\n",
       "      <td>Dodge D-50</td>\n",
       "      <td>What did Mitsubishi rename its Forte to?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>Luther's rediscovery of \"Christ and His salvat...</td>\n",
       "      <td>Christ and His salvation</td>\n",
       "      <td>What became the foundation of the Reformation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>BSkyB has no veto over the presence of channel...</td>\n",
       "      <td>Ofcom</td>\n",
       "      <td>Who does BSkyB have an operating license from?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9650</th>\n",
       "      <td>Research by Harvard economist Robert Barro, fo...</td>\n",
       "      <td>Harvard</td>\n",
       "      <td>What institution does Robert Barro hail from?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9651</th>\n",
       "      <td>LeGrande writes that \"the formulation of a sin...</td>\n",
       "      <td>specific</td>\n",
       "      <td>Le grand concludes that an author's words offe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9652 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context  \\\n",
       "0     Private schools generally prefer to be called ...   \n",
       "1     Private schools, also known as independent sch...   \n",
       "2     The Pilgrim Street building was refurbished be...   \n",
       "3     Chris Keates, the general secretary of Nationa...   \n",
       "4     In Berlin, the Huguenots created two new neigh...   \n",
       "...                                                 ...   \n",
       "9647  Compact trucks were introduced, such as the To...   \n",
       "9648  Luther's rediscovery of \"Christ and His salvat...   \n",
       "9649  BSkyB has no veto over the presence of channel...   \n",
       "9650  Research by Harvard economist Robert Barro, fo...   \n",
       "9651  LeGrande writes that \"the formulation of a sin...   \n",
       "\n",
       "                                           answer  \\\n",
       "0                                          public   \n",
       "1                                     independent   \n",
       "2                                           three   \n",
       "3     child protection and parental rights groups   \n",
       "4                                         1806-07   \n",
       "...                                           ...   \n",
       "9647                                   Dodge D-50   \n",
       "9648                     Christ and His salvation   \n",
       "9649                                        Ofcom   \n",
       "9650                                      Harvard   \n",
       "9651                                     specific   \n",
       "\n",
       "                                               question  \n",
       "0     What schools do preparatory schools prepare Br...  \n",
       "1     Along with non-governmental and nonstate schoo...  \n",
       "2     How many cinemas are currently housed at one s...  \n",
       "3     A statement made by Chris Keates caused issues...  \n",
       "4            What years did this occupation take place?  \n",
       "...                                                 ...  \n",
       "9647           What did Mitsubishi rename its Forte to?  \n",
       "9648     What became the foundation of the Reformation?  \n",
       "9649     Who does BSkyB have an operating license from?  \n",
       "9650      What institution does Robert Barro hail from?  \n",
       "9651  Le grand concludes that an author's words offe...  \n",
       "\n",
       "[9652 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:51:53.844718: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 14:51:55.053983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-27 14:51:55.054032: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-27 14:51:59.680081: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-27 14:51:59.681521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-27 14:51:59.681561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/sanatan/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filepath, max_len_inp=512,max_len_out=96):\n",
    "        self.path = filepath\n",
    "\n",
    "        self.passage_column = \"context\"\n",
    "        self.answer = \"answer\"\n",
    "        self.question = \"question\"\n",
    "\n",
    "        # self.data = pd.read_csv(self.path)\n",
    "        self.data = pd.read_csv(self.path,nrows=1000)\n",
    "\n",
    "        self.max_len_input = max_len_inp\n",
    "        self.max_len_output = max_len_out\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.skippedcount =0\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels [labels==0] = -100\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            passage,answer,target = self.data.loc[idx, self.passage_column],self.data.loc[idx, self.answer], self.data.loc[idx, self.question]\n",
    "\n",
    "            input_ = \"context: %s  answer: %s </s>\" % (passage, answer)\n",
    "            target = \"question: %s </s>\" % (str(target))\n",
    "\n",
    "            # get encoding length of input. If it is greater than self.max_len skip it\n",
    "            test_input_encoding = self.tokenizer.encode_plus(input_,\n",
    "                                        truncation=False,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "            length_of_input_encoding = len(test_input_encoding['input_ids'][0])\n",
    "\n",
    "\n",
    "            if length_of_input_encoding > self.max_len_input:\n",
    "              self.skippedcount = self.skippedcount + 1\n",
    "              continue\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len_input, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len_output, pad_to_max_length=True,return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0062a4cfb37348d1ac58ad27ba2838e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:219: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504878499aa34aec9c55a39a25c0ede2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:219: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "/home/sanatan/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = QuestionGenerationDataset(t5_tokenizer,'dataset/train.csv')\n",
    "validation_dataset = QuestionGenerationDataset(t5_tokenizer,'dataset/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = t5_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: President Donald Trump said and predicted that some states would reopen this month. answer: Donald Trump </s>\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "context =\"President Donald Trump said and predicted that some states would reopen this month.\"\n",
    "answer = \"Donald Trump\"\n",
    "text = \"context: \"+context + \" \" + \"answer: \" + answer + \" </s>\"\n",
    "print (text)\n",
    "\n",
    "encoding = t5_tokenizer.encode_plus(text,max_length =512, padding=True, return_tensors=\"pt\")\n",
    "print (encoding.keys())\n",
    "input_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "Tast True\n"
     ]
    }
   ],
   "source": [
    "t5_model.eval()\n",
    "beam_outputs = t5_model.generate(\n",
    "    input_ids=input_ids,attention_mask=attention_mask,\n",
    "    max_length=72,\n",
    "    early_stopping=True,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=3\n",
    "\n",
    ")\n",
    "\n",
    "for beam_output in beam_outputs:\n",
    "    sent = t5_tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/sanatan/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator12recordStreamERKNS_7DataPtrENS0_10CUDAStreamE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42)\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self,hparams, t5model, t5tokenizer):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        # self.hparams = hparams\n",
    "        self.model = t5model\n",
    "        self.tokenizer = t5tokenizer\n",
    "\n",
    "\n",
    "    def forward( self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n",
    "         outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "         return outputs\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_input_ids = batch[\"target_ids\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        self.log('train_loss',loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_input_ids = batch[\"target_ids\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        self.log(\"val_loss\",loss)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.hparams.batch_size,num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset, batch_size=self.hparams.batch_size,num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sanatan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f6d5dab8c44ac3bd010f949ac76a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4371e9f66b374fe78a80ff06054510a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args_dict = dict(\n",
    "    batch_size = 4,\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "model = T5FineTuner(args,t5_model,t5_tokenizer)\n",
    "trainer = pl.Trainer(max_epochs = 1)\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanatan/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e5c27ab0be4991b43d62f79c72af84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc0d63942bf4fb99bc1bf7611a4ed7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e390ae597a174db88f30f8cba25093c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20248e831aa463b9ebd712e59828722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distractors: []\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "def generate_distractors(context, question, answer):\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    masked_question = question.replace(answer, \"[MASK]\")\n",
    "    encoded_input = tokenizer.encode(context, masked_question, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(encoded_input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(encoded_input)\n",
    "        predictions = output[0]\n",
    "        masked_token_predictions = predictions[0, mask_token_index, :]\n",
    "\n",
    "    predicted_token_ids = torch.argmax(masked_token_predictions, dim=1)\n",
    "    predicted_tokens = [tokenizer.decode(token_id.item()) for token_id in predicted_token_ids]\n",
    "\n",
    "    distractors = [token for token in predicted_tokens if token.lower() != answer.lower()]\n",
    "    return distractors\n",
    "\n",
    "context = \"The context sentence.\"\n",
    "question = \"What is the capital of France?\"\n",
    "answer = \"Paris\"\n",
    "\n",
    "distractors = generate_distractors(context, question, answer)\n",
    "print(\"Distractors:\", distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 21:59:53.958265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 21:59:55.352611: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-08 21:59:55.353414: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-08 22:00:15.550250: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-08 22:00:15.551937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-08 22:00:15.551982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/sanatan/.local/lib/python3.8/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk('s2v_old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_same_sense_words(original,wordlist):\n",
    "  filtered_words=[]\n",
    "  base_sense =original.split('|')[1] \n",
    "  print (base_sense)\n",
    "  for eachword in wordlist:\n",
    "    if eachword[0].split('|')[1] == base_sense:\n",
    "      filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
    "  return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_similarity_score(wordlist,wrd):\n",
    "  score=[]\n",
    "  for each in wordlist:\n",
    "    similarity_ratio = SequenceMatcher(None, each.lower(), wrd.lower()).ratio()\n",
    "    score.append(similarity_ratio)\n",
    "  return max(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense2vec_get_words(word,s2v,topn,question):\n",
    "    output = []\n",
    "    print (\"word \",word)\n",
    "    try:\n",
    "      sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
    "      most_similar = s2v.most_similar(sense, n=topn)\n",
    "      # print (most_similar)\n",
    "      output = filter_same_sense_words(sense,most_similar)\n",
    "      print (\"Similar \",output)\n",
    "    except:\n",
    "      output =[]\n",
    "\n",
    "    threshold = 0.6\n",
    "    final=[word]\n",
    "    checklist =question.split()\n",
    "    for x in output:\n",
    "      if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
    "        final.append(x)\n",
    "    \n",
    "    return final[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphrase\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/sanatan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  stone\n",
      "NOUN\n",
      "Similar  ['Stones', 'Iron', 'Same Stone', 'Granite', 'Marble', 'Obsidian', 'Wood', 'Anvil', 'Slabs', 'Solid Stone', 'Only Stone', 'Smooth Stone', 'Big Stone', 'Wooden Planks', 'Black Stone', 'Stone Wall', 'Rocks', 'Adamantine', 'Slab', 'Stone Floor', 'Crystal', 'Large Stone', 'Jewel', 'Stone Slab', 'Lava', 'Other Stone', 'Anvils', 'Cement', 'Stone Block', 'Cauldron', 'Flint', 'Ingot', 'Mithril', 'Gemstones', 'Stonework', 'Feather', 'Large Stones', 'Stone Walls']\n",
      "distractors  ['Iron', 'Granite', 'Marble', 'Obsidian', 'Wood', 'Anvil', 'Slabs', 'Smooth Stone', 'Wooden Planks', 'Rocks', 'Adamantine', 'Crystal', 'Jewel', 'Lava', 'Cement', 'Cauldron', 'Flint', 'Mithril', 'Feather', 'Large Stones']\n",
      "['Rocks', 'Anvil', 'Wooden planks', 'Feather', 'Mithril']\n"
     ]
    }
   ],
   "source": [
    "def get_distractors (word,origsentence,sense2vecmodel,sentencemodel,top_n,lambdaval):\n",
    "  distractors = sense2vec_get_words(word,sense2vecmodel,top_n,origsentence)\n",
    "  print (\"distractors \",distractors)\n",
    "  if len(distractors) ==0:\n",
    "    return distractors\n",
    "  distractors_new = [word.capitalize()]\n",
    "  distractors_new.extend(distractors)\n",
    "  # print (\"distractors_new .. \",distractors_new)\n",
    "\n",
    "  embedding_sentence = origsentence+ \" \"+word.capitalize()\n",
    "  # embedding_sentence = word\n",
    "  keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
    "  distractor_embeddings = sentencemodel.encode(distractors_new)\n",
    "\n",
    "  # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors,4,0.7)\n",
    "  max_keywords = min(len(distractors_new),5)\n",
    "  filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
    "  # filtered_keywords = filtered_keywords[1:]\n",
    "  final = [word.capitalize()]\n",
    "  for wrd in filtered_keywords:\n",
    "    if wrd.lower() !=word.lower():\n",
    "      final.append(wrd.capitalize())\n",
    "  final = final[1:]\n",
    "  return final\n",
    "\n",
    "sent = \"What is found in the earth's crust?\"\n",
    "keyword = \"stone\"\n",
    "\n",
    "print (get_distractors(keyword,sent,s2v,sentence_transformer_model,40,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-rake==1.4.4\n",
      "  Downloading python-rake-1.4.4.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: python-rake\n",
      "  Building wheel for python-rake (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-rake: filename=python_rake-1.4.4-py3-none-any.whl size=13459 sha256=4a9bbd10d09160fd8afb630d584afe16e835942bfd62370ee56f6060c0dd69af\n",
      "  Stored in directory: /home/sanatan/.cache/pip/wheels/7a/dd/2f/e16099449134869d4a9a96c94092dc0101d7a3cc25c309f8e2\n",
      "Successfully built python-rake\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: python-rake\n",
      "Successfully installed python-rake-1.4.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-rake==1.4.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
